PCA
Principal Component Analysis (PCA) reduces the dimensionality of a large dataset, 
by identifying the hyperplane that lies closet to the data, and then it projects 
the data onto it. Statistically speaking, it uses an orthogonal transformation to 
convert a set of instances of possibly correlated variables into a set of values of 
linearly uncorrelated variables. This transformation comes at the expense of accuracy. 
The idea behind this popular dimensionality reduction is to trade a little accuracy for
 simplicity by preserving as much information as possible. PCA identifies some linear 
 combinations of initial variables that are called Principal Components. 
 These combinations are constructed in such a way that they (i.e., principal component) 
 are uncorrelated and capture the most of the variance within the training set by 
 squeezing and compressing the information within the initial variables into the first 
 components. In other words, for a data set with n observations and p predictors, 
 PCA creates at most min(n-1,p) principal components that the first one accounts for 
 the largest amount of variance, the second one carries the largest amount of remaining 
 variance, and so on. Therefore, the proportion of the dataset’s variance each 
 component carries diminishes as PCA creates more components.

MCA
Multiple Correspondence Analysis (MCA) reduces the categorical features by creating a 
matrix with values of zero or one. If a categorical variable has more than two 
different classes, this method binarized it. For instance, a color feature including 
blue, red, and green is categorized in such a way that blue becomes [1,0,0], red 
becomes [0,1,0] and green becomes [0,0,1]. By using this method, MCA creates a matrix 
that consists of individual x variables where the rows represent individuals and 
the columns are dummy variables. Applying a standard correspondence analysis on this 
matrix is the next step. The result is a linear combination of rows that carries the 
most possible information of all categorical features.

WHAT IS FACTORIAL ANALYSIS OF MIXED DATA? 
Factorial analysis of mixed data is a method initially developed by Hill and Smith (1972).
A few variants of this method have been developed since then (Escofier 1979, Pagès 2004).
 The method used in Xlstat is called PCAmix and was developed by Chavent et al (2014). 
 This method can be seen as a mixture of two popular methods of factorial analysis: 
 Principal Component Analysis (PCA) which allows to study an observations/quantitative 
 variables table and Multiple Correspondence Analysis (MCA) which allows to study an 
 observations/qualitative variables table.

Similarly to other factorial analysis methods, PCAmix aims to reduce data dimensionality
 as well as to identify nearness between variables but also proximity between the 
 observations.

To do so, it does a series of statistical transformations, including calculations of 
the correlation matrix, eigenvalues and eigenvectors, on a set of qualitative and/or 
quantitative variables in order to project them on a vector space generated by 
orthogonal components. The number of principal components is chosen depending on the 
explained percentage of variance of the model by each component. This is when the 
dimensional reduction occurs because a small number of components will be enough to 
explain a high percentage of the variance.

The observations and the variables end up being represented as points in orthogonal 
two-dimensional spaces. Sometimes, only the first and second components are necessary 
to explain a large percentage of the variance and so you will be able analyze a 
two-dimensional projection of what was initially dozens of variables.